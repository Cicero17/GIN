{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import OneHotDegree\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A class implementing the MLP layer of a GIN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, input_dim, hidden_dim):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            num_layers: An integer indicating the number of layers. Input layer is not counted.\n",
    "            input_dim: An integer indicating the input dimension.\n",
    "            hiddem_dim: An integer indicating the size of the hidden layers. Also indicates the output dimension.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        \n",
    "        if self.num_layers == 0:\n",
    "            self.linears.extend([nn.Linear(input_dim, hidden_dim)])\n",
    "            self.batch_norms.extend([nn.BatchNorm1d(hidden_dim)])\n",
    "        else:\n",
    "            self.linears = nn.ModuleList([nn.Linear(input_dim, hidden_dim)] + \n",
    "                                         [nn.Linear(hidden_dim, hidden_dim) for _ in range(self.num_layers - 1)])\n",
    "            self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(self.num_layers)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of the model on input.\n",
    "        \n",
    "        Args:\n",
    "            x: A Tensor representing the input.\n",
    "        \n",
    "        Returns: \n",
    "            x: A Tensor representing the output.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(max(1, self.num_layers)):\n",
    "            x = self.linears[i](x)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\" Reset the parameters of the model. \"\"\"\n",
    "        \n",
    "        for i in range(max(1, self.num_layers)):\n",
    "            self.linears[i].reset_parameters()\n",
    "            self.batch_norms[i].reset_parameters()\n",
    "                \n",
    "\n",
    "class GIN(nn.Module):\n",
    "    \"\"\"A class implementing the GIN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, num_mlps, input_dim, hidden_dim, output_dim, dropout_rate, \n",
    "                 nbh_agg, graph_agg, learn_eps, random):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            num_layers: An integer indicating the number of layers. Input layer is counted.\n",
    "            num_mlps: An integer indicating the number of MLP layers. If is 0, the model will be a GIN 1-LAYER.\n",
    "            input_dim: An integer indicating the input dimension.\n",
    "            output_dim: An integer indicating the input dimension.\n",
    "            hiddem_dim: An integer indicating the size of the hidden layers.\n",
    "            dropout_rate: A float between 0. and 1., indicating the dropout rate, applied at the final layer.\n",
    "            nbh_agg: A string in ['sum', 'mean', 'max', 'lstm'] indicating the type of node neighborhood aggregation.\n",
    "            graph_agg: A string in ['sum'] indicating the readout.\n",
    "            learn_eps: A boolean indicating whether the epsilon parameter is learnable or is fixed as a zero tensor.\n",
    "            random: A boolean indicating whether random initialization is used. If True, the input dimension is double,\n",
    "                the input features random features from a normal distribution are concatenated.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_mlps = num_mlps\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nbh_agg = nbh_agg\n",
    "        self.graph_agg = graph_agg\n",
    "        self.learn_eps = learn_eps\n",
    "        self.random = random\n",
    "        if self.learn_eps:\n",
    "            self.eps = nn.Parameter(torch.zeros(self.num_layers - 1))\n",
    "        else:\n",
    "            self.eps = torch.zeros(self.num_layers - 1)\n",
    "            \n",
    "        if self.random:\n",
    "            self.input_dim *= 2\n",
    "\n",
    "        self.mlps = nn.ModuleList([MLP(self.num_mlps, self.input_dim, self.hidden_dim)] + \n",
    "                                  [MLP(self.num_mlps, self.hidden_dim, self.hidden_dim) for _ in range(\n",
    "                                      self.num_layers - 2)])\n",
    "        \n",
    "        self.linears_graph = nn.ModuleList([nn.Linear(self.input_dim, self.output_dim)] + \n",
    "                                           [nn.Linear(self.hidden_dim, self.output_dim) for _ in range(\n",
    "                                               self.num_layers - 1)])\n",
    "        \n",
    "        if nbh_agg == 'lstm':\n",
    "            self.lstm_input = nn.LSTM(input_dim, input_dim, batch_first = True)\n",
    "            self.lstm_hidden = nn.LSTM(hidden_dim, hidden_dim, batch_first = True)\n",
    "        \n",
    "    def __adj_matrix(self, batch):\n",
    "        \"\"\" Obtain the node adjacency matrix of a graph. An adjacency matrix adj_matrix satisfies: \n",
    "        adj_matrix[i][j] = 1. if nodes i and j are adjacen; 0. otherwise.\n",
    "        \n",
    "        Args:\n",
    "            batch: A batch corresponding to a graph.\n",
    "            \n",
    "        Returns: A sparse tensor representing the adjacency matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_nodes = batch.x.shape[0]\n",
    "        return torch.sparse_coo_tensor(batch.edge_index, [1.] * batch.edge_index.shape[1], (num_nodes, num_nodes))\n",
    "    \n",
    "    def __nbh_indices(self, adj_matrix):\n",
    "        \"\"\" Obtain a list containing the tensor of neighbor indices for each node. nbh_indices satisfies: \n",
    "        nbh_indices[i] = torch.tensor([j | j neighbour of i]).\n",
    "        \n",
    "        Args: \n",
    "            adj_matrix: A tensor representing the adjacency matrix of a graph.\n",
    "            \n",
    "        Returns:\n",
    "            nbh_indices: A list of tensors representing the neighbors indices.\n",
    "        \"\"\"\n",
    "        \n",
    "        nbh_indices = []\n",
    "        for i in range(adj_matrix.shape[0]):\n",
    "            nbh_indices.append(adj_matrix[i].coalesce().indices()[0])\n",
    "        return nbh_indices\n",
    "        \n",
    "    def __graphs_matrix(self, batch):\n",
    "        \"\"\" Obtain the graphs matrix given a batch which is a graph formed from other graphs. A graphs_matrix satisfies: \n",
    "        graphs_matrix[g][n] = 1. if graph g has node n; 0. otherwise.\n",
    "        \n",
    "        Args:\n",
    "            batch: A batch corresponding to a graph.\n",
    "        \n",
    "        Returns:\n",
    "            graphs_matrix: A tensor representing the graphs matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_graphs = batch.y.shape[0]\n",
    "        num_nodes = batch.x.shape[0]\n",
    "        \n",
    "        graphs_matrix = torch.zeros((num_graphs, num_nodes))\n",
    "        for node in range(num_nodes):\n",
    "            graphs_matrix[batch.batch[node]][node] = 1.\n",
    "        return graphs_matrix\n",
    "        \n",
    "    def __nbh_agg(self, x, adj_matrix, nbh_indices = None, k = None):\n",
    "        \"\"\" Obtain neighborhood aggregation tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: A tensor containing node features.\n",
    "            adj_matrix: adj_matrix: A tensor representing the adjacency matrix.\n",
    "            nbh_indices: A list of tensors representing the neighbors indices, used in max and LSTM aggregation.\n",
    "            k: An integer indicating the layer index, used in LSTM aggregation.\n",
    "        \n",
    "        Returns: A tensor representing the neighborhoods aggregations.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.nbh_agg == 'sum':\n",
    "            return torch.spmm(adj_matrix, x)\n",
    "\n",
    "        if self.nbh_agg == 'mean':\n",
    "            ones = torch.ones(adj_matrix.shape[0], 1)\n",
    "            num_neighbours = torch.spmm(adj_matrix, ones)\n",
    "            return torch.spmm(adj_matrix, x) / num_neighbours\n",
    "\n",
    "        if self.nbh_agg == 'max':\n",
    "            max_agg = []\n",
    "            for i in range(x.shape[0]):\n",
    "                if nbh_indices[i].shape[0] != 0:\n",
    "                    index = nbh_indices[i].expand((x.shape[1], nbh_indices[i].shape[0])).t()\n",
    "                    nbhs = x.gather(0, index)\n",
    "                    index = torch.arange(nbhs.shape[1]).expand(nbhs.shape[0], nbhs.shape[1])\n",
    "                    max_agg.append(scatter_max(nbhs.flatten(), index.flatten())[0])\n",
    "                else:\n",
    "                    max_agg.append(torch.zeros(x.shape[1]))\n",
    "            return torch.stack(max_agg)\n",
    "\n",
    "        if self.nbh_agg == 'lstm':\n",
    "            lstm_agg = []\n",
    "            for i in range(x.shape[0]):\n",
    "                if nbh_indices[i].shape[0] != 0:\n",
    "                    index = nbh_indices[i].expand((x.shape[1], nbh_indices[i].shape[0])).t()\n",
    "                    nbhs = x.gather(0, index)\n",
    "                    nbhs = nbhs[torch.randperm(nbhs.shape[0])]\n",
    "                    nbhs = nbhs.reshape(1, nbhs.shape[0], nbhs.shape[1])\n",
    "                    if k == 0:\n",
    "                        output = self.lstm_input(nbhs)[0]\n",
    "                    else:\n",
    "                        output = self.lstm_hidden(nbhs)[0]\n",
    "                    lstm_agg.append(output[0][-1])\n",
    "                else:\n",
    "                    lstm_agg.append(torch.zeros(x.shape[1]))\n",
    "            return torch.stack(lstm_agg)\n",
    "            \n",
    "    def __graph_agg(self, x, graphs_matrix):\n",
    "        \"\"\" Obtain graph aggregation tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: A tensor containing node features.\n",
    "            graphs_matrix: A tensor representing the graphs matrix.\n",
    "            \n",
    "        Returns: A tensor representing the graph readout.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.graph_agg == 'sum':\n",
    "            return torch.spmm(graphs_matrix, x)\n",
    "        \n",
    "    def forward(self, batch):  \n",
    "        \"\"\" Forward pass of the model on input.\n",
    "        \n",
    "        Args:\n",
    "            batch: A batch, representing a  graph containing all the graphs part of the same batch.\n",
    "    \n",
    "        Returns: \n",
    "            output: A Tensor representing the output.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = batch.x\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        if self.random:\n",
    "            x = torch.cat((x, torch.empty((x.shape[0], x.shape[1])).normal_(mean=0.,std=1.)), axis = 1)\n",
    "            \n",
    "        num_graphs = batch.y.shape[0]\n",
    "        graphs_matrix = self.__graphs_matrix(batch)\n",
    "        adj_matrix = self.__adj_matrix(batch)\n",
    "        nbh_indices = None\n",
    "        if self.nbh_agg in ['max', 'lstm']:\n",
    "            nbh_indices = self.__nbh_indices(adj_matrix)\n",
    "        \n",
    "        output = torch.zeros((num_graphs, self.output_dim))\n",
    "        graph_agg = self.__graph_agg(x, graphs_matrix)\n",
    "        output += F.dropout(self.linears_graph[0](graph_agg), self.dropout_rate, training = self.training)\n",
    "            \n",
    "        for k in range(self.num_layers - 1):\n",
    "            x = (1 + self.eps[k]) * x + self.__nbh_agg(x, adj_matrix, nbh_indices, k)\n",
    "            x = self.mlps[k](x)       \n",
    "            graph_agg = self.__graph_agg(x, graphs_matrix)\n",
    "            output += F.dropout(self.linears_graph[k + 1](graph_agg), self.dropout_rate, training = self.training)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\" Reset the parameters of the model. \"\"\"\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.mlps[i].reset_parameters()\n",
    "        for i in range(self.num_layers):\n",
    "            self.linears_graph[i].reset_parameters()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(k, model, dataset, epochs, batch_size, output_dim = None):\n",
    "    \"\"\" Run k fold cross validation. \n",
    "    \n",
    "    Args:\n",
    "        k: An integer indicating the number of folds.\n",
    "        model: A pytorch module.\n",
    "        dataset: A dataset.\n",
    "        epochs: An integer indicating the number of epochs.\n",
    "        batch_size: An integer indicating the size of batches.\n",
    "        output_dim: An integer indicating the output dimension.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(dataset)\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        model.reset_parameters()\n",
    "        train_data_loader = DataLoader(dataset[:i * N // k] + dataset[(i + 1) * N // k:], batch_size = batch_size, \n",
    "                                       shuffle = True)\n",
    "        test_data_loader = DataLoader(dataset[i * N // k: (i + 1) * N // k], batch_size = 1, shuffle = False)\n",
    "        \n",
    "        train(model, train_data_loader, epochs) \n",
    "        print(model.eps)\n",
    "        acc = test(model, test_data_loader, output_dim)\n",
    "        print('Fold ' + str(i + 1) + ' accuracy: ' + str(acc * 100) + '%')\n",
    "        accuracies.append(acc)\n",
    "    print()\n",
    "    accuracies = torch.tensor(accuracies)\n",
    "    print('Global accuracy: ' + str(torch.mean(accuracies).item() * 100) + '%')\n",
    "    print('Standard deviation: ' + str(torch.std(accuracies).item() * 100) + '%')\n",
    "    \n",
    "def train(model, data_loader, epochs):\n",
    "    \"\"\" Train the model on data for a given number of epochs.\n",
    "    \n",
    "    Args:\n",
    "        model: A pytorch module.\n",
    "        data_loader: A data loader corresponding to a dataset.\n",
    "        epochs: An integer indicating the number of epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    loss =  nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma = 0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            cur_loss = loss(model(batch), batch.y)\n",
    "            cur_loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "            \n",
    "def test(model, data_loader, output_dim = None):\n",
    "    \"\"\" Evaluate the model on data.\n",
    "    \n",
    "    Args:\n",
    "        model: A pytorch module.\n",
    "        data_loader: A data loader corresponding to a dataset.\n",
    "        output_dim: An integer indicating the output dimension. If is None, then it's considered to be 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dim is None: # binary by default\n",
    "        output_dim = 2\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for batch in data_loader:\n",
    "        labels.append(batch.y.item())\n",
    "        predictions.append(torch.argmax(model(batch)[0]).item())\n",
    "    print('Predictions:' , end = ' ')\n",
    "    for i in range(output_dim):\n",
    "        print(str(predictions.count(i)), end = ' ')\n",
    "    print()\n",
    "    labels = torch.tensor(labels)\n",
    "    predictions = torch.tensor(predictions)\n",
    "    return (torch.sum(labels == predictions) / labels.shape[0]).item()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset):\n",
    "    \"\"\"  \n",
    "    Args:\n",
    "        dataset: A dataset.\n",
    "    \n",
    "    Returns: The dataset after it's permuted.\n",
    "    \"\"\"\n",
    "    \n",
    "    index = list(range(len(dataset)))\n",
    "    random.shuffle(index)\n",
    "    return [dataset[i] for i in index]\n",
    "\n",
    "def add_node_features(dataset, num_node_features):\n",
    "    \"\"\" Add zero tensors as node features to a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: A dataset.\n",
    "        num_node_features: An integer indicating the number of node features.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        num_nodes = torch.max(dataset[i].edge_index) + 1\n",
    "        new_dataset.append(Data(x = torch.zeros((num_nodes,  num_node_features)), edge_index = dataset[i].edge_index, \n",
    "                                y = dataset[i].y))\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mutag = TUDataset(os.path.join(cwd, 'Datasets'), 'MUTAG') # input_dim = 7\n",
    "\n",
    "dataset_proteins = TUDataset(os.path.join(cwd, 'Datasets'), 'PROTEINS') # input_dim = 3\n",
    "\n",
    "dataset_imdb_b = TUDataset(os.path.join(cwd, 'Datasets'), 'IMDB-BINARY', transform = OneHotDegree(max_degree = 540))\n",
    "\n",
    "dataset_imdb_m = TUDataset(os.path.join(cwd, 'Datasets'), 'IMDB-MULTI', transform = OneHotDegree(max_degree = 352)) \n",
    "\n",
    "dataset_rdt_b = TUDataset(os.path.join(cwd, 'Datasets'), 'REDDIT-BINARY') \n",
    "dataset_rdt_b = add_node_features(dataset_rdt_b, 64)\n",
    "\n",
    "dataset_rdt_m = TUDataset(os.path.join(cwd, 'Datasets'), 'REDDIT-MULTI-5K') \n",
    "dataset_rdt_m = add_node_features(dataset_rdt_b, 64)\n",
    "\n",
    "dataset_collab = TUDataset(os.path.join(cwd, 'Datasets'), 'COLLAB') \n",
    "dataset_collab = add_node_features(dataset_collab, 64)\n",
    "\n",
    "dataset_nci1 = TUDataset(os.path.join(cwd, 'Datasets'), 'NCI1') # input_dim = 37\n",
    "\n",
    "dataset_ptc = TUDataset(os.path.join(cwd, 'Datasets'), 'PTC_MR') # input_dim = 18\n",
    "\n",
    "dataset_syntheticnew = TUDataset(os.path.join(cwd, 'Datasets'), 'SYNTHETICnew', \n",
    "                                 transform = OneHotDegree(max_degree = 18))\n",
    "\n",
    "dataset_cuneiform = TUDataset(os.path.join(cwd, 'Datasets'), 'Cuneiform') # input_dim = 3\n",
    "\n",
    "num_node_features = 64\n",
    "datasets = {'MUTAG': (dataset_mutag, 7, 2),\n",
    "            'PROTEINS': (dataset_proteins, 3, 2), \n",
    "            'IMDB-BINARY': (dataset_imdb_b, 541, 2),\n",
    "            'IMDB-MULTI': (dataset_imdb_m, 353, 3),\n",
    "            'REDDIT-BINARY': (dataset_rdt_b, num_node_features, 2),\n",
    "            'REDDIT-MULTI-5K': (dataset_rdt_b, num_node_features, 5),\n",
    "            'COLLAB': (dataset_collab, num_node_features, 3),\n",
    "            'NCI1': (dataset_nci1, 37, 2),\n",
    "            'PTC_MR': (dataset_ptc, 18, 2),\n",
    "            'SYNTHETICnew': (dataset_syntheticnew, 19, 2),\n",
    "            'Cuneiform': (dataset_cuneiform, 3, 30)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.3252, -0.2164, -0.0030,  0.0417], requires_grad=True)\n",
      "Predictions: 6 12 \n",
      "Fold 1 accuracy: 83.33333134651184%\n",
      "Parameter containing:\n",
      "tensor([-0.5209, -0.2829, -0.1203,  0.4685], requires_grad=True)\n",
      "Predictions: 4 15 \n",
      "Fold 2 accuracy: 84.21052694320679%\n",
      "Parameter containing:\n",
      "tensor([-0.7635, -0.2579, -0.0443,  0.9134], requires_grad=True)\n",
      "Predictions: 6 13 \n",
      "Fold 3 accuracy: 84.21052694320679%\n",
      "Parameter containing:\n",
      "tensor([-1.1456, -0.2362, -0.0925,  1.1386], requires_grad=True)\n",
      "Predictions: 3 16 \n",
      "Fold 4 accuracy: 89.47368264198303%\n",
      "Parameter containing:\n",
      "tensor([-1.4640, -0.2330, -0.1603,  1.3404], requires_grad=True)\n",
      "Predictions: 8 11 \n",
      "Fold 5 accuracy: 89.47368264198303%\n",
      "Parameter containing:\n",
      "tensor([-1.5799, -0.3840,  0.0194,  1.8077], requires_grad=True)\n",
      "Predictions: 4 14 \n",
      "Fold 6 accuracy: 88.88888955116272%\n",
      "Parameter containing:\n",
      "tensor([-1.6540, -0.4309, -0.6354,  2.1429], requires_grad=True)\n",
      "Predictions: 9 10 \n",
      "Fold 7 accuracy: 94.73684430122375%\n",
      "Parameter containing:\n",
      "tensor([-1.6764, -0.4844, -1.0886,  2.2732], requires_grad=True)\n",
      "Predictions: 10 9 \n",
      "Fold 8 accuracy: 73.68420958518982%\n",
      "Parameter containing:\n",
      "tensor([-1.6803, -0.4727, -1.3090,  2.2044], requires_grad=True)\n",
      "Predictions: 6 13 \n",
      "Fold 9 accuracy: 94.73684430122375%\n",
      "Parameter containing:\n",
      "tensor([-1.7473, -0.4152, -1.3821,  2.1175], requires_grad=True)\n",
      "Predictions: 6 13 \n",
      "Fold 10 accuracy: 84.21052694320679%\n",
      "\n",
      "Global accuracy: 86.69590950012207%\n",
      "Standard deviation: 6.224477663636208%\n"
     ]
    }
   ],
   "source": [
    "(dataset, input_dim, output_dim) = datasets['MUTAG']\n",
    "model = GIN(5, 2, input_dim, 32, output_dim, 0.5, 'sum', 'sum', True, False)\n",
    "k_fold_cross_validation(10, model, dataset, 350, 32, output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
